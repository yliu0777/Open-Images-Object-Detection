{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is a test implementation using ResNet50 for object detection copied from the link below\n",
    "https://www.kaggle.com/shivamb/objects-bounding-boxes-using-resnet50-imageai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import csv\n",
    "import random\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.utils.data_utils import GeneratorEnqueuer\n",
    "import matplotlib.pyplot as plt\n",
    "from imageai.Detection import ObjectDetection\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 99999 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# The image path must contain a folder which contains the images...\n",
    "# So image_path = \"../input/open-images-2019-object-detection/test/\" doesn't work\n",
    "image_path = \"../input/open-images-2019-object-detection/\"\n",
    "batch_size = 100\n",
    "#ImageDataGenerator document: https://keras.io/preprocessing/image/\n",
    "img_generator = ImageDataGenerator().flow_from_directory(image_path, shuffle=False, batch_size = batch_size)\n",
    "n_rounds = math.ceil(img_generator.samples / img_generator.batch_size)\n",
    "filenames = img_generator.filenames\n",
    "\n",
    "img_generator = GeneratorEnqueuer(img_generator)\n",
    "img_generator.start()\n",
    "img_generator = img_generator.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\yawen\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model_weight_path = \"../input/resnet50/yolo.h5\"\n",
    "detector = ObjectDetection()\n",
    "detector.setModelTypeAsYOLOv3()\n",
    "detector.setModelPath(model_weight_path)\n",
    "detector.loadModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('eggs.csv', newline='') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
    "    for row in spamreader:\n",
    "        print(', '.join(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ImageAi Detection source code: https://github.com/OlafenwaMoses/ImageAI/blob/master/build/lib/imageai/Detection/__init__.py\n",
    "<br>object detection example: https://github.com/OlafenwaMoses/ImageAI/blob/master/examples/object_detection.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teddy bear  :  65.874582529068  :  (279, 378, 437, 597)\n",
      "cell phone  :  89.60634469985962  :  (497, 312, 591, 444)\n",
      "person  :  57.99713730812073  :  (71, 424, 291, 707)\n",
      "person  :  83.3885908126831  :  (477, 324, 602, 483)\n",
      "backpack  :  68.57743859291077  :  (398, 450, 630, 682)\n",
      "person  :  83.26128125190735  :  (400, 170, 480, 475)\n",
      "person  :  89.31005001068115  :  (646, 181, 761, 630)\n",
      "person  :  98.11328053474426  :  (891, 206, 1024, 663)\n",
      "person  :  98.3992874622345  :  (424, 136, 581, 661)\n",
      "person  :  99.08872246742249  :  (553, 244, 753, 682)\n",
      "person  :  99.48728084564209  :  (733, 360, 930, 682)\n",
      "person  :  99.84135031700134  :  (74, 52, 444, 673)\n",
      "car  :  77.26734280586243  :  (331, 349, 475, 426)\n",
      "person  :  64.69390392303467  :  (394, 600, 446, 681)\n",
      "person  :  73.9486813545227  :  (13, 482, 93, 613)\n",
      "person  :  76.78179740905762  :  (253, 552, 320, 665)\n",
      "person  :  77.27376818656921  :  (514, 615, 580, 682)\n",
      "person  :  78.50961089134216  :  (593, 625, 642, 682)\n",
      "person  :  97.60264754295349  :  (325, 564, 444, 681)\n",
      "person  :  98.04016947746277  :  (47, 516, 146, 681)\n",
      "person  :  99.25824999809265  :  (140, 500, 291, 680)\n",
      "handbag  :  55.95272183418274  :  (400, 333, 476, 443)\n",
      "person  :  50.987404584884644  :  (640, 198, 675, 262)\n",
      "person  :  51.828062534332275  :  (801, 191, 838, 284)\n",
      "person  :  60.079360008239746  :  (384, 162, 435, 336)\n",
      "person  :  65.18740057945251  :  (935, 189, 968, 268)\n",
      "person  :  67.23517775535583  :  (214, 178, 252, 240)\n",
      "person  :  71.84078097343445  :  (310, 146, 367, 229)\n",
      "person  :  72.11464047431946  :  (895, 195, 934, 281)\n",
      "person  :  73.5444962978363  :  (4, 169, 47, 265)\n",
      "person  :  74.74856376647949  :  (118, 186, 181, 246)\n",
      "person  :  77.49455571174622  :  (956, 189, 990, 264)\n",
      "person  :  89.89403247833252  :  (853, 190, 892, 290)\n",
      "person  :  91.52851104736328  :  (28, 158, 117, 316)\n",
      "person  :  94.57542300224304  :  (237, 161, 320, 417)\n",
      "person  :  96.3884174823761  :  (981, 207, 1023, 329)\n",
      "person  :  98.48830103874207  :  (397, 121, 632, 598)\n",
      "person  :  99.222731590271  :  (518, 159, 638, 550)\n",
      "person  :  99.63297247886658  :  (67, 237, 240, 645)\n",
      "person  :  99.80837106704712  :  (260, 215, 429, 608)\n",
      "person  :  99.96798038482666  :  (632, 138, 798, 525)\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(filenames)\n",
    "for i in range(5):\n",
    "    detections = detector.detectObjectsFromImage(image_path + filenames[i], output_image_path= os.path.join(\"../input/open-images-2019-object-detection/\", f\"{i}_output.jpg\"),\n",
    "                                                 minimum_percentage_probability=50)\n",
    "    for eachObject in detections:\n",
    "        print(eachObject[\"name\"] , \" : \", eachObject[\"percentage_probability\"], \" : \", eachObject[\"box_points\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    batch = next(img_generator)\n",
    "    image_arr, _ = batch\n",
    "    \n",
    "detections = detector.detectObjectsFromImage(input_image=, output_image_path=os.path.join(execution_path , \"2_detected.jpg\"), minimum_percentage_probability=40)\n",
    "for eachObject in detections:\n",
    "    print(eachObject[\"name\"] , \" : \", eachObject[\"percentage_probability\"], \" : \", eachObject[\"box_points\"] )\n",
    "    print(\"--------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
